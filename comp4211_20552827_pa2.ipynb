{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7V9CkxqcdcA"
      },
      "source": [
        "# AutoregressiveModel\n",
        "\n",
        "**Description:** AutoregressiveModel implemented in Keras to generate image.\n",
        "\n",
        "**Objective:** The objective of this assignment is to practise using the TensorFlow machine learning framework\n",
        "through implementing custom training modules and data reader modules for image generation on\n",
        "the Chinese Calligraphy dataset using a convolutional neural network (CNN) based architecture.\n",
        "Throughout the assignment, students will be guided to develop the CNN-based model step by\n",
        "step and study how to build custom modules on TensorFlow and the effects of different model\n",
        "configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5qADsjVcdcG"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Image generation is one of the fundamental computer vision tasks, referring to the process of generating new images that are visually realistic and similar to real-world images. It is widely used in many applications, such as super resolution, photograph editing and 3D modelling. \n",
        "\n",
        "One approach to image generation is to use models that learn to predict the probability distribution of pixel values, given the values of all the previous pixels. These models generate images one pixel at a time, using the previously generated pixels to condition the generation of the next pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6pmZLclieHE"
      },
      "source": [
        "### Setting environment\n",
        "\n",
        "Note: You can only use the packages listed below !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nzuClBlOcdcI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "from PIL import Image\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2uXmIzcdcK"
      },
      "source": [
        "## Getting the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-ajC_RQxam"
      },
      "source": [
        "### Download dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cPs8yqGRNWix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed59551-181d-48d6-ad4b-3c08b007aea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-03 13:02:10--  https://docs.google.com/uc?export=download&confirm=t&id=18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.200.102, 74.125.200.113, 74.125.200.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.200.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lm3kn7mi1jql2m85vodokh9j3b1fcvvk/1680526875000/03446042882167617448/*/18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt?e=download&uuid=9ede8c3b-f1ba-474e-80a7-019960e3b198 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-04-03 13:02:11--  https://doc-08-9k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lm3kn7mi1jql2m85vodokh9j3b1fcvvk/1680526875000/03446042882167617448/*/18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt?e=download&uuid=9ede8c3b-f1ba-474e-80a7-019960e3b198\n",
            "Resolving doc-08-9k-docs.googleusercontent.com (doc-08-9k-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-08-9k-docs.googleusercontent.com (doc-08-9k-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75267160 (72M) [application/x-zip-compressed]\n",
            "Saving to: ‘calligraphy.zip’\n",
            "\n",
            "calligraphy.zip     100%[===================>]  71.78M   263MB/s    in 0.3s    \n",
            "\n",
            "2023-04-03 13:02:11 (263 MB/s) - ‘calligraphy.zip’ saved [75267160/75267160]\n",
            "\n",
            "mkdir: cannot create directory ‘./data’: File exists\n",
            "^C\n",
            "replace ./data/train_half/00000.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: total 1392\n",
            "drwxr-xr-x 2 root root  270336 Mar  2 21:35 test_half\n",
            "drwxr-xr-x 2 root root 1146880 Feb 28 12:38 train_half\n"
          ]
        }
      ],
      "source": [
        "# Download dataset from google drive\n",
        "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt\" -O calligraphy.zip && rm -rf /tmp/cookies.txt\n",
        "! mkdir ./data && unzip -q calligraphy.zip -d ./data/ && rm calligraphy.zip\n",
        "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1w7JVXz6U-NVDZxBf1oSAVjKdR4BJs1zI' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1w7JVXz6U-NVDZxBf1oSAVjKdR4BJs1zI\" -O calligraphy.zip && rm -rf /tmp/cookies.txt\n",
        "! unzip -q calligraphy.zip -d ./data/ && rm calligraphy.zip\n",
        "! ls -l ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrivlNhIRIgp"
      },
      "source": [
        "### make dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWlGYkLdRGYo",
        "outputId": "80b73f1f-d00e-4e33-eacf-9e5ef63af453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42000\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "input_shape = (32, 32, 1)\n",
        "batch_size = 32\n",
        "data_dir = \"./data/train_half\"\n",
        "data_name = \"calligraphy\"\n",
        "test_dir = \"/content/data/test_half\"\n",
        "!ls ./data/train_half | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVekI5LcdcL"
      },
      "source": [
        "## Create layers for the requisite Layers for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "adlqUIs5VXZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b91bfa-72d6-4991-dfe8-32c853b1fade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1313 (32, 32, 32, 1) (32, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# dataset class\n",
        "def binarize(arr, img):\n",
        "\n",
        "  #initialize threshold\n",
        "  thresh=0.33\n",
        "\n",
        "  #convert image to greyscale\n",
        "  img=img.convert('L') \n",
        "\n",
        "  width,height=img.size\n",
        "\n",
        "  #traverse through pixels \n",
        "  for x in range(width):\n",
        "    for y in range(height):\n",
        "\n",
        "      #if intensity less than threshold, assign white\n",
        "      if arr[x][y] < thresh:\n",
        "        arr[x][y] = 0\n",
        "\n",
        "      #if intensity greater than threshold, assign black \n",
        "      else:\n",
        "        arr[x][y] = 1\n",
        "\n",
        "  return arr\n",
        "\n",
        "class CalligraphySequence(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,image_dir, batch_size, start = 0.0, last = 1.0,):\n",
        "        ### [C1: Build init and len functions]\n",
        "        # Your code here\n",
        "        self.image_dir = image_dir\n",
        "        size_of_dir = len(os.listdir(image_dir))\n",
        "        self.files = os.listdir(image_dir)[int(size_of_dir* start): int(last*size_of_dir)]\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self): #counting the number of files in directory\n",
        "        ### [C1: Build init and len functions]\n",
        "        # Your code here\n",
        "      count = 0\n",
        "      for paths in os.listdir(self.image_dir): #os.listdir(director) is a list of files in directory and can access using listdir \n",
        "        if os.path.isfile(os.path.join(self.image_dir, paths)):\n",
        "          count += 1\n",
        "      return math.ceil(count / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ### [C2: Build getitem function]a\n",
        "        # Round all pixel values less than 33% of the max 256 value to 0\n",
        "        # anything above this value gets rounded up to 1 so that all values are either\n",
        "        # 0 or 1\n",
        "        # Your code here\n",
        "        thres_val = 0.33 \n",
        "        low = idx * self.batch_size\n",
        "        high = min(low + self.batch_size, len(self.files))\n",
        "        file_y = self.files[low:high] #file names in batches \n",
        "        batches = list()\n",
        "        for file_name in file_y:\n",
        "          converted = Image.open(os.path.join(self.image_dir,file_name)).convert('L').resize((32, 32))\n",
        "          batches.append(converted)\n",
        "        normalized = []\n",
        "        for image in batches:\n",
        "          val = (np.array(image) - np.min(np.array(image)))/ (np.max(np.array(image)) - np.min(np.array(image)))\n",
        "          normalized.append(val)\n",
        "        images = list()\n",
        "        for array in normalized:\n",
        "          val = binarize(array, Image.fromarray(array))\n",
        "          images.append(val)\n",
        "        batch_y =  np.array(images)#binarize the normalized images  \n",
        "        batch_y.resize((32, 32, 32, 1))\n",
        "        return batch_y, batch_y\n",
        "# final shape should be 1313 (32, 32, 32, 1) (32, 32, 32, 1)\n",
        "train_ds = CalligraphySequence(data_dir, batch_size)\n",
        "print(len(train_ds), train_ds[0][0].shape, train_ds[0][1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK-wOq1WkaAv"
      },
      "source": [
        "### Given function for conv2d / down_shift / right_shift / concat_elu\n",
        "1. conv2d: 2d convolution layer using layers.Conv2D\n",
        "\n",
        "2. down_shift: shift feature down in height dimension (by padding zero to the top and drop the bottom)\n",
        "\n",
        "3. right_shift: shift feature right in width dimension\n",
        "\n",
        "4. concat_elu: a nonlinearity layer (http://arxiv.org/abs/1603.05201)\n",
        "\n",
        "The down_shift and right_shift functions are used to avoid information leaks in a causal network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EDUbrK8XkZr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ac7540-0b68-4a85-d59e-ef7144317fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 32, 64)\n"
          ]
        }
      ],
      "source": [
        "class Conv2d(layers.Layer):\n",
        "    def __init__(self, num_filters, filter_size=[3, 3], stride=[1, 1], pad='SAME', nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = layers.Conv2D(num_filters, filter_size, padding = pad, strides = stride, activation = nonlinearity, \n",
        "                         kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05))\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "def down_move(x, step=1):\n",
        "    input_shape = tf.shape(x)\n",
        "    return tf.concat([tf.zeros((input_shape[0], step, input_shape[2], input_shape[3])), x[:, :input_shape[1] - step, :, :]], 1)\n",
        "\n",
        "def right_move(x, step=1):\n",
        "    input_shape = tf.shape(x)\n",
        "    return tf.concat([tf.zeros((input_shape[0], input_shape[1], step, input_shape[3])), x[:, :, :input_shape[2] - step, :]], 2)\n",
        "\n",
        "def concat_elu(x):\n",
        "    \"\"\" like concatenated ReLU (http://arxiv.org/abs/1603.05201), but then with ELU \"\"\"\n",
        "    axis = len(x.get_shape()) - 1\n",
        "    out = tf.nn.elu(tf.concat([x, -x], axis))\n",
        "    return out\n",
        "inputs = (32, 32, 32, 1)\n",
        "x = tf.random.normal(inputs)\n",
        "y = tf.keras.layers.Conv2D(64, 5, padding=\"same\", strides = 1, input_shape=inputs[1:])(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgKeYdBploK-"
      },
      "source": [
        "### Gated Residual Block\n",
        "The GatedResnet class applies gated residual connections to input tensors for feature extraction.\n",
        "\n",
        "Please follow Section 4.2.3 to implement coding question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fy9VWOO0mDHs"
      },
      "outputs": [],
      "source": [
        "class DownMovedConv2d(layers.Layer):\n",
        "    def __init__(self, num_filters, filter_size=[2, 3], stride=[1, 1], pad='VALID', nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C4: Build DownMovedConv2d.]\n",
        "        # Your code here\n",
        "        self.conv2d = layers.Conv2D(num_filters, filter_size, stride,pad, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05))\n",
        "        self.filter_size = filter_size\n",
        "    \n",
        "    def call(self, x):\n",
        "        ### [C4: Build DownMovedConv2d.]\n",
        "        # Your code here\n",
        "        padtop = self.filter_size[0] - 1\n",
        "        padside = (self.filter_size[1] - 1) // 2\n",
        "        paddings = tf.constant([[0,0], [padtop, 0], [padside, padside],[0,0]])\n",
        "        x = tf.pad(x, paddings, 'CONSTANT')\n",
        "        return self.conv2d(x)\n",
        "class DownRightMovedConv2d(layers.Layer):\n",
        "    def __init__(self, num_filters, filter_size=[2, 2], stride=[1, 1], pad='VALID', nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C3: Build DownRightMovedConv2d.]\n",
        "        # Your code here\n",
        "        self.conv2d = layers.Conv2D(num_filters, filter_size, stride,pad, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05))\n",
        "        self.filter_size = filter_size\n",
        "    def call(self, x):\n",
        "        ### [C3: Build DownRightMovedConv2d.]\n",
        "        # Your code here\n",
        "        padtop = self.filter_size[0] -1 \n",
        "        padside = self.filter_size[1] - 1\n",
        "        paddings = tf.constant([[0,0], [padtop, 0], [padside, 0], [0,0]])\n",
        "        x = tf.pad(x, paddings, 'CONSTANT')\n",
        "        return self.conv2d(x)\n",
        "\n",
        "class TensorDense(layers.Layer):\n",
        "    def __init__(self, num_units, nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C5: Build TensorDense.]\n",
        "        # Your code here\n",
        "        self.num_units = num_units\n",
        "        self.dense = layers.Dense(units = num_units, kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05))\n",
        "    def call(self, x):\n",
        "        ### [C5: Build TensorDense.]\n",
        "        # Your code here\n",
        "        shapes = tf.shape(x)\n",
        "        B = shapes[0]\n",
        "        H = shapes[1]\n",
        "        W = shapes[2]\n",
        "        C = shapes[3]\n",
        "        x = tf.reshape(x, [B*H*W, C])\n",
        "        x = self.dense(x)\n",
        "        x = tf.reshape(x, [B,H,W, self.num_units])\n",
        "        return x\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "class GatedResnet(layers.Layer):\n",
        "    def __init__(self, num_filters, nonlinearity=concat_elu, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C6: Build GatedResnet.]\n",
        "        # Your code here\n",
        "        self.num_filters = num_filters\n",
        "        self.nonlinearity = nonlinearity\n",
        "        self.nnLayer_1 = DownRightMovedConv2d(num_filters = self.num_filters)\n",
        "        self.nnLayer_2 = DownRightMovedConv2d(num_filters = self.num_filters * 2)\n",
        "    def call(self, x):\n",
        "        ### [C6: Build GatedResnet.]\n",
        "        # Your code here\n",
        "        input_shape = tf.shape(x)\n",
        "        C = input_shape[3]\n",
        "      #make residual\n",
        "        residual_1 = x\n",
        "        x = self.nonlinearity(x) #nonlinearity activation session \n",
        "        x = self.nnLayer_1(x)\n",
        "        x = self.nonlinearity(x)\n",
        "        x = self.nnLayer_2(x)\n",
        "        F, G= tf.split(x, [C, C], axis = 3)\n",
        "        G = tf.keras.activations.sigmoid(G)\n",
        "        F = tf.math.multiply(F, G) #should be added to tensor flow with residual\n",
        "        return tf.math.add(F, residual_1) #returns the shape of (B,H,W,C_in)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFo2cKKoKtv"
      },
      "source": [
        "### Main AutoregressiveModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s8RHOpB_mHQP"
      },
      "outputs": [],
      "source": [
        "class AutoregressiveModel(layers.Layer):\n",
        "    def __init__(self, n_resnet=5, n_filters=256, n_block=12, n_output=10, **kwargs):\n",
        "        super().__init__()\n",
        "        self.n_resnet = n_resnet #n\n",
        "        self.n_filters = n_filters #n^filters\n",
        "        self.n_block = n_block #m\n",
        "        self.n_output = n_output#n^output\n",
        "        # init all network layers\n",
        "        self.down_moved_conv2d = DownMovedConv2d(num_filters=self.n_filters, filter_size=[1, 3])\n",
        "        self.down_right_moved_conv2d = DownRightMovedConv2d(num_filters=self.n_filters, filter_size=[2, 1])\n",
        "        ### [C7: Build AutoregressiveModel.]\n",
        "        # Your code here3\n",
        "        self.out_dense = TensorDense(num_units = self.n_output)\n",
        "        self.ul_list_gated_resnet = [] #gatedresnet n\n",
        "        self.ul_list_dense_layer = []#networkblock m \n",
        "        for i in range(n_block): #M times \n",
        "          gatedresnetlist = []\n",
        "          for j in range(n_resnet):   #N times \n",
        "            gatedresnetlist.append(GatedResnet(num_filters = n_filters))\n",
        "          self.ul_list_dense_layer.append(TensorDense(num_units = n_filters)) \n",
        "          self.ul_list_gated_resnet.append(gatedresnetlist) #M x N object\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        x = down_move(self.down_moved_conv2d(inputs)) + right_move(self.down_right_moved_conv2d(inputs)) #tensorsum (B,H,C, N^filter)\n",
        "        ### [C7: Build AutoregressiveModel.]\n",
        "        # Your code here\n",
        "        for i in range(self.n_block):\n",
        "          for j in range(self.n_resnet):\n",
        "             x = self.ul_list_gated_resnet[i][j](x)\n",
        "          x = self.ul_list_dense_layer[i](x)\n",
        "        x = tf.keras.activations.elu(x)\n",
        "        x = self.out_dense(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VJgCyzmzu4T_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61972fcc-7b37-405f-eb77-d7c000871897"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEF1CtrPcdcO"
      },
      "source": [
        "## Build the model based on the original paper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mZzvQW8jcdcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117d9d47-955b-44db-a5b2-c75fd21c1cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " autoregressive_model (Autor  (None, 32, 32, 10)       3571914   \n",
            " egressiveModel)                                                 \n",
            "                                                                 \n",
            " conv2d_75 (Conv2D)          (None, 32, 32, 1)         11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,571,925\n",
            "Trainable params: 3,571,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " autoregressive_model (Autor  (None, 32, 32, 10)       3571914   \n",
            " egressiveModel)                                                 \n",
            "                                                                 \n",
            " conv2d_75 (Conv2D)          (None, 32, 32, 1)         11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,571,925\n",
            "Trainable params: 3,571,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "## Build the model based on the original paper\n",
        "inputs = keras.Input(shape=input_shape, dtype=tf.float32)\n",
        "x = AutoregressiveModel(n_resnet=6, n_filters=64, n_block=6, n_output=10)(inputs)\n",
        "out = keras.layers.Conv2D(\n",
        "    filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\"\n",
        ")(x)\n",
        "\n",
        "pixel_cnn = keras.Model(inputs, out)\n",
        "pixel_cnn.summary()\n",
        "### [C11: Model training and log reporting]\n",
        "# you can use keras.optimizers.Adam here to define \"adam\"\n",
        "# compile your model and make a summary on its architecture\n",
        "# Your code here\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.0001,beta_1=0.95, beta_2=0.9995, epsilon=1e-6, use_ema=True, ema_momentum=0.9995 )\n",
        "pixel_cnn.compile(\n",
        "    optimizer=adam,\n",
        "     loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "pixel_cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3ruvOoJtQyDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50026e75-bf7f-4ff2-e348-a2639faa49f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " autoregressive_model (Autor  (None, 32, 32, 10)       3571914   \n",
            " egressiveModel)                                                 \n",
            "                                                                 \n",
            " conv2d_75 (Conv2D)          (None, 32, 32, 1)         11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,571,925\n",
            "Trainable params: 3,571,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "### [C8: Load the pretrained weights]\n",
        "# Your code here\n",
        "model_weight = pixel_cnn.load_weights(filepath = \"/content/drive/MyDrive/pixel_cnn_e5.h5\")\n",
        "pixel_cnn.summary()\n",
        "test_dir = \"/content/data/test_half\"\n",
        "train_ds = CalligraphySequence(data_dir, batch_size, start = 0.0, last = 0.8)\n",
        "val_ds = CalligraphySequence(data_dir, batch_size, start = 0.8, last = 1.0)\n",
        "test_ds = CalligraphySequence(image_dir = test_dir, batch_size = batch_size)\n",
        "call_backs =keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/frequency\", save_weights_only = True, monitor = 'val_accuracy',mode = 'max', save_best_only = True),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOocGHbkPlsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f03b713-c3da-4f24-8575-fdf8d23e24b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 833/1313 [==================>...........] - ETA: 30s - loss: 0.1498 - accuracy: 0.9382"
          ]
        }
      ],
      "source": [
        "\n",
        "### [C11: Model training and log reporting]\n",
        "# you can use model.fit here\n",
        "# Your code here\n",
        "#pixel_cnn.fit(x = train_ds, validation_data = val_ds, epochs = epoch, verbose = 1, batch_size = 32, callbacks = call_backs, use_multiprocessing = True, workers = 4)\n",
        "pixel_cnn.fit(x = train_ds, validation_data = val_ds, epochs = 10, batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH4M5H4KOh31"
      },
      "outputs": [],
      "source": [
        "# save weights\n",
        "pixel_cnn.save_weights('pixel_cnn_e15.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTlv-hdhcdcQ"
      },
      "source": [
        "## Demonstration\n",
        "\n",
        "The AutoregressiveModel cannot generate the full image at once. Instead, it must generate each pixel in\n",
        "order, append the last generated pixel to the current image, and feed the image back into the\n",
        "model to repeat the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBULc1N8cdcS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "820401fb-a1d8-4520-b232-001a92bf975f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [01:06<00:00,  2.07s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA60lEQVR4nN1VSRLEIAgMqfz/y8zBKkIhS0v0MpxUlmaXmPk6SfdR6/8NQESlMiLjAyCaoKQDUOoQ0ZBhZmbO5R2AsnGHXX1NMCgxR5RxtVjiXNFFeDFCCzPmbFRkhDVeJEQpyQywNgcGCUngkziOIMk1Ug8j2LUE3wh0+KYL5awzo+uRRP9EjNm6yyqL0Syyflme5AZGIl+kKCfju4vUBDATl5APUG4hvIk/fTifAFZpeZK3WL+iGsgQ4cWMyEZgNnCPtGe3YbgKrvvREJjlYVMEpiLpY/PuyyEjClJ/m4Lkt+mwu+XPOf6j/QCgzpk0fWOweAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA9ElEQVR4nM1WQQ7DMAiDaf//MjtERYiAgSiVxqmNUjvYQMoiQm/G51X0FgEzg9cLBCIyBZ0RgAiJmdmunxA4CEz5nUITUVZ4im43DEzGToToLYISEaDTSKL18cJiZltdoFsPTbbPWLqZyZQcdiUU7u9mUJ4029DqZABXzsqxRDuruh2StSTSL3cRyjE1rqK9/LFWKUFomraC8wA4kXqA3XMegJ13+gDEH1yZK+wgGkUhkUNUmr5QRQbyBCY+J9hxw2vgDoHNBvS2P5NNPxMXXCyWoDWLsmnzbhWpLNnQ716Z7pjZLHIruGQD0UdlXkZQRXd/6H+qUZZsOdcZcAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA/UlEQVR4nM2WSw7DMAhEQ9T7X5kunKSIzzBGilRWdYp5Y2OwRVWPN+18Nfp/A0SEcfuMAY5R5XIbkAoHJ2V7i2IsfA5LANjiFVFvw4LQClKGiJDpvfxTCTF7Lqj9iBeRAxypEsF0AcqJp0abFFoMLbcNAW4mGEYGBVDV9tRWdqaKnCgynynsfKLYH9bPzYkwXBa+F1VKH3DLSwDjUmjnHnySGbcjayQNIN2B9kRZh/4+WAzXnfh+lwPSoGRENzHfoi2NUZwd9lv0aHf5AOuzwxKwFkHWFDjKaAVWO/gXK+jrYJyMZXvPlkHNb1w4s2cyCxg/wn+A6s7D/bV1/gJic6tU8N3GWwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA8klEQVR4nLVWQQ7DMAgb0/7/ZXaoGjGwIbCGwzTR1jaG0Iqqvk7G+yh6m0BEROQgwSDkdA8+/zxs7WJCH7OItaeoINeoqmXP6x44CHb/dVu8Si1aJesdkK8MSsCU2vwO2XxMIXrDokFArTWBnb/4h+FSgli47TBDv36haaACMQFFRcqEA1vkRhPK3+TwBAzXoUCxkLUxRRYxPwH26nBMEwNdfpcgrporwzbVyjcqcNI2t5NEURELPIaOCwRpWARP72oG82r3lZnUlFv0U8Fgs5auzrepRUyUgZPc5YhjYjOgggFHEk++cHbX9Tjw8jj96fgFlFK3GmV3We0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Create an empty array of pixels.\n",
        "batch =  10 # you may want to change this parameter \n",
        "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
        "batch, rows, cols, channels = pixels.shape\n",
        "\n",
        "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
        "for row in tqdm(range(rows)):\n",
        "    for col in range(cols):\n",
        "        for channel in range(channels):\n",
        "            ### [C9: Qualitative Evaluation] \n",
        "            # Your code here\n",
        "            # 1. Feed the whole array and retrieving the pixel value probabilities for the next\n",
        "            # pixel. You can use model.predict function to get predict value for each pixel.\n",
        "            y_pred = pixel_cnn.predict(pixels, verbose = 0)\n",
        "            # 2. Use the probabilities to pick pixel values and append the values to the image\n",
        "            # frame. you can use tf.math.ceil to achieve the 0.5 threshold.\n",
        "            y_pred = y_pred[:,row, col, channel]\n",
        "            pixels[:, row, col, channel] = np.random.binomial(1, y_pred)\n",
        "def deprocess_image(x):\n",
        "    # Stack the single channeled black and white image to RGB values.\n",
        "    x = np.stack((x, x, x), 2)\n",
        "    # Undo preprocessing\n",
        "    x *= 255.0\n",
        "    # Convert to uint8 and clip to the valid range [0, 255]\n",
        "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
        "    return x\n",
        "\n",
        "\n",
        "# Iterate over the generated images and plot them with matplotlib.\n",
        "for i, pic in enumerate(pixels):\n",
        "    keras.preprocessing.image.save_img(\n",
        "        \"generated_image_{}.png\".format(i), deprocess_image(np.squeeze(pic, -1))\n",
        "    )\n",
        "\n",
        "display(Image(\"generated_image_0.png\"))\n",
        "display(Image(\"generated_image_1.png\"))\n",
        "display(Image(\"generated_image_2.png\"))\n",
        "display(Image(\"generated_image_3.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLCjV00RT78w"
      },
      "source": [
        "## Quantitative Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG8oF4E_UPIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35da2c92-3457-4ab6-fbe8-97e0c67d52f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "329/329 [==============================] - 16s 35ms/step - loss: 0.1851 - accuracy: 0.9233\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1850622445344925, 0.9233470559120178]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "### [C10: Quantitative Evaluation]\n",
        "# Your code here\n",
        "pixel_cnn.evaluate(test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}